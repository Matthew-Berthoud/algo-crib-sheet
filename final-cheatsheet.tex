\documentclass[8pt]{minimal}
\usepackage{graphicx} 
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{lipsum} % For dummy text

% 0.1 inch margins for printing
\textwidth 8.3truein
\textheight 10.8truein
\oddsidemargin -0.9in
\topmargin -0.9in

\parindent 0pt
\def\baselinestretch{0.9}

\begin{document}
\begin{flushleft}
\begin{multicols}{3}


    \textbf{1. Mathematical Foundations}

    \textbf{1.1 Common Functions}

$e^x=1+x+{x^2\over 2!}+{x^3\over 3!}+\cdots$ ($e \approx 2.718$)

$\lim_{n\to\infty}{(1+{x\over n})}^n=e^x$.

$\log(ab)=\log a+\log b$, $\log ({a\over b})=\log a-\log b$.

$\log_a b={{\log_c b}\over {\log_c a}}$.

$\log_a b^n=n\log_a b\not={(\log_a b)}^n$, $a^{\log_a n}=n$,
$a^{\log_c b}=b^{\log_c a}$.

$ln(1+x)=x-{x^2\over 2}+{x^3\over 3}-{x^4\over 4}+\cdots$.

$n!=n\cdot (n-1)!$, $0!=1$.

Sterling's approximation: $n!=\sqrt{2\pi n}({n\over e})^n(1+
\Theta({1\over n}))$. (Note: $\Theta$ means having the same order of 
magnitude.) The following approximation also holds:
$n!=\sqrt{2\pi n}({n\over e})^ne^{\alpha_n}$, where
${1\over 12n+1}<\alpha_n<{1\over 12n}$.

$\log n!=\Theta(n\log n)$.

Log star function: how many times log must be applied before the result is $\le 1$:

$\log^* n=\min\{i\ge0:\log^{(i)}n\le1\}$

$\log^*2=1$, $\log^* 4=2$, $\log^*16=3$, $\log^*65536=4$, $\log^*2^{65536}=5$

    \textbf{1.2 Asymptotic Notation}

% Big vs. Little O explained: https://stackoverflow.com/questions/1364444/difference-between-big-o-and-little-o-notation

\item $f(n)=O(g(n))$ if $\exists c, n_0$ such that 
$f(n)\le cg(n)$ for $n\ge n_0$.

\item $f(n)=\Omega(g(n))$ if $\exists c, n_0$ such that
$f(n)\ge cg(n)$ for $n\ge n_0$.

\item $f(n)=\Theta(g(n))$ if $\exists c_1, c_2, n_0$ such that
$c_1g(n)\le f(n)\le c_2g(n)$ for $n\ge n_0$.

\item $f(n)=o(g(n))$ if $\forall c$ $\exists n_0$ such that
$f(n)<cg(n)$ for $n\ge n_0$. 

\item $f(n)=\omega(g(n))$ if $\forall c$ $\exists n_0$ such that 
$f(n)>cg(n)$ for $n\ge n_0$.


An alternative definition for $f(n)=o(g(n))$ is 
$\lim_{n\to\infty}{f(n)\over g(n)}=0$. Likewise, an alternative definition
for $f(n)=\omega(g(n))$ is $\lim_{n\to\infty}{f(n)\over g(n)}=\infty$.

{\em Rule of thumb:} constant $\le$ polylogarithmic $\le$ polynomial
$\le$ exponential $\le$ superexponential.

{\em Example:} $1$, $\sqrt{\log n}$, $\ln n$, $(\log n)^2$, $\sqrt n$,
$\sqrt n \log n$, $10n$, $n\log n$, $n^2$, $n^{\log\log n}$,
$2^n$, $n2^n$, $n!$, $2^{2^n}$.

Taking logarithms helps: $f(n)=O(g(n))$ iff $\log f(n)=O(\log g(n))$.


    \textbf{1.3 Summations/Series}

Arithmetic: $\sum_{i=1}^{n} i = {1\over2}n(n+1)$

Geometric: $\sum_{i=0}^{n} = {{r^{n+1}-1}\over{r-1}}$ for
$r\not=1$

\;\;\;\;\;\;\;\;\;\;\;\;\,\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;$={1\over{1-r}}$ for $|r|<1$.

Harmonic: $1+{1\over2}+\cdots+{1\over n}
=\ln n+\gamma+{\epsilon\over{2n}}$

for $\gamma=0.5772156649\ldots$
(Euler's constant) and $0<\epsilon<1$.


Binomial: ${n\choose 0}+{n\choose 1}+{n\choose 2}+
\cdots+{n\choose n}=2^n$.

Other useful sums:

$\sum_{i=1}^n i^2={1\over6}n(n+1)(2n+1)$.

$\sum_{i=1}^n i^3=(\sum_{i=1}^n i)^2$.

$\sum_{i=1}^n ix^{i-1}={{nx^{n+1}-(n+1)x^n+1}\over{(x-1)^2}}$.


    \textbf{1.4 Proof Techniques}

Contradiction

Induction: basis, hypothesis, step



    \textbf{1.5 Solving Recurrences}

Iteration: Appy recurrence until can figure out summation

{\em Example:} $T(n)=3T({n\over4})+n$.

Assume $n=4^k$, so $k=\log_4 n$.
\begin{eqnarray*}
T(n)&=&3T({n\over4})+n\\
&=&3^2T({n\over{4^2}})+{3\over4}n+n\\
&=&3^3T({n\over{4^3}})+({3\over4})^2n+({3\over4})n+n\\
&=&\cdots\\
&=&3^kT({n\over{4^k}})+(({3\over4})^{k-1}+\cdots+({3\over4})+1)n\\
&=&3^{\log_4 n}+{{1-({3\over4})^{\log_4 n}}\over{1-{3\over4}}}n\\
&=&4n-3\cdot 3^{\log_4 n}\\
&=&O(n).
\end{eqnarray*}


Recursion Tree:

Master: does not cover all cases

{\em Master Theorem:} If $T(n)=aT({n\over b})+f(n)$ for $a\ge1$ and $b>1$, then

(a) if $f(n)=O(n^{(\log_ba)-\epsilon})$ for some $\epsilon>0$, then
$T(n)=\Theta(n^{\log_ba})$;

(b) if $f(n)=\Theta(n^{\log_ba})$, then
$T(n)=\Theta(n^{\log_ba}\log n)$;

(c) if $f(n)=\Omega(n^{(\log_ba)+\epsilon})$ for $\epsilon>0$ and if
$af({n\over b})\le cf(n)$ for $c<1$ and all large $n$, then
$T(n)=\Theta(f(n))$.


{\em Example:} $T(n)=3T({n\over4})+n\log n$.

$a=3$, $b=4$, and $f(n)=n\log n$.
Case (c) applies. So $T(n)=\Theta(n\log n)$.

Substitution:


    \textbf{2. Analysis of Algorithms}

    % \textbf{2.1 Overview}

    \textbf{2.2 Worst-Case Analysis}
    
Insertion\_Sort($A$)
\par\quad for $j\leftarrow 2$ to $n$
\par\quad\quad $key\leftarrow A[j]$
\par\quad\quad $i\leftarrow j-1$
\par\quad\quad while $i>0$ and $A[i]>key$
\par\quad\quad\quad $A[i+1]\leftarrow A[i]$
\par\quad\quad\quad $i\leftarrow i-1$
\par\quad\quad $A[i+1]\leftarrow key$

To insert $A[j]$ into the sorted $A[1\ldots j-1]$, the algorithm
will make at most $j-1$ comparisons and $j-1$ shifts. So the overall 
time complexity,
which is dominated by the number of comparisons and the number of
shifts, is at most $2\sum_{j=2}^n(j-1)
=\Theta(n^2)$.



Euclid's algorithm (300 B.C.): Greatest common divisor.

If $n\not=0$, then $gcd(m,n)=gcd(n, m \bmod n)$;

If $n=0$, then $gcd(m,n)=m$.

Euclid($m,n$)\qquad //Assume $m\ge n$
\par\quad while $n>0$
\par\quad\quad $t\leftarrow m\bmod n$
\par\quad\quad $m\leftarrow n$
\par\quad\quad $n\leftarrow t$
\par\quad return $m$

The time complexity of the algorithm is determined by the number of iterations
of the while-loop. Let it be $k$. Also let $m_i$ and $n_i$ be the values of
$m$ and $n$ at the end of the $i$th iteration, respectively.

We observe that (1) $n_k=0$, $n_i\ge 1$ for $i<k$, and 
$n_0>n_1>\cdots>n_k$; (2) $m_i=n_{i-1}$, which implies that $m_i\ge n_i$ 
for $i\ge1$; (3) $n_i=m_{i-1} \bmod n_{i-1}<{m_{i-1}\over2}={n_{i-2}\over2}$
for $i\ge2$. (Note: If $a\ge b$, then $a\bmod b<{a\over2}$.)

We then have $1\le n_{k-1}<{n_{k-3}\over2}<{n_{k-5}\over2^2}<\cdots<
{(n_1,n_0)\over2^{\lceil k/2\rceil-1}}\le {n_0\over2^{\lceil k/2\rceil-1}}$.

Thus, ${k\over2}-1\le\lceil{k\over2}\rceil-1<\log n_0$,
which implies $k\le 2+2\log n_0$.

So $T(m,n)=O(k)=O(\log n)$.



Multiplying two integers $x$ and $y$: 

Assume that $x$ (multiplicand) and $y$ (multiplier) 
have the same number of figures, $n=2^k$,
denoted $(x)_n$ and $(y)_n$.
Then $(x)_n=10^{n\over2}\cdot(a)_{n\over2}+(b)_{n\over2}$ and
$(y)_n=10^{n\over2}\cdot(c)_{n\over2}+(d)_{n\over2}$.

So $(x)_n\cdot(y)_n=10^n((a)_{n\over2}\cdot (c)_{n\over2})+
10^{n\over2}((a)_{n\over2}\cdot (d)_{n\over2}+ (b)_{n\over2}\cdot
(c)_{n\over2})+(b)_{n\over2}\cdot (d)_{n\over2}$. The problem of 
multiplying two integers of size $n$ is now reduced to four subproblems
of multiplying two integers of size $n\over2$.

$T(n)=4T({n\over2})+n=\Theta(n^2)$ by the master theorem.

Now let $s=(a)_{n\over2}\cdot (c)_{n\over2}$,
$t=(b)_{n\over2}\cdot (d)_{n\over2}$, and
$r=((a)_{n\over2}+(b)_{n\over2})\cdot((c)_{n\over2}+(d)_{n\over2})$.
Then $(x)_n\cdot(y)_n=10^ns+10^{n\over2}(r-s-t)+t$. Only three
subproblems have to be solved in the recursion.

$T(n)=3T({n\over2})+n=\Theta(n^{\log_23})=\Theta(n^{1.585})$ 
by the master theorem.


    \textbf{2.3 Average-Case Analysis}

Insertion sort:

Assumptions: (1) Distinct items; (2) Each permutation with equal
probability to occur.

$T(n)=$ average \# of pairwise comparisons $=\sum_{j=2}^n c_j$, where
$c_j$ is the average \# of comparisons to insert $A[j]$ ($key$) into the sorted
$A[1\ldots j-1]$. For $key$, there are $j$ possible positions, with 
probability ${1\over j}$ for each.

So $c_j={1\over j}\cdot 1+{1\over j}\cdot 2+\cdots+{1\over j}\cdot (j-1)
+{1\over j}\cdot (j-1)={1\over2}(j+1)-{1\over j}$.

Therefore, $T(n)=\sum_{j=2}^n({1\over2}(j+1)-{1\over j})
={1\over4}n^2+{3\over4}n-H_n=\Theta(n^2)$.


Binary search

Given $A[1\ldots n]$ sorted and $x$ (a query). Is $x$ a member of $A$?


Assumptions: (1) $x\in A$; (2) Distinct items in $A$; (3)
$Pr(x=A[i])={1\over n}$ for any $i=1,2,\ldots,n$; (4) $n=2^k-1$ for some $k$.

Binary search $\Rightarrow$ Decision tree of $n$ nodes (full binary tree
with $k$ levels).

Average-case time $\Rightarrow$ Average length of a path from the root to
a tree node.

\begin{center}
\begin{tabular}{l|l|l}
Level & \# of nodes & average length\\ \hline
1 & 1 & ${1\over n}\cdot 1$\\
2 & 2 & ${2\over n}\cdot 2$\\
3 & 4 & ${4\over n}\cdot 3$\\
$\cdots$ & $\cdots$ & $\cdots$\\
$k$ & $2^{k-1}$ & ${2^{k-1}\over n}\cdot k$
\end{tabular}
\end{center}

\begin{eqnarray*}
T(n)&=&\sum_{i=1}^k {2^{i-1}\over n}\cdot i\\
&=&{1\over n}\sum_{i=1}^k i\cdot 2^{i-1}\\
&=&{1\over n}(k\cdot 2^k-2^k+1)\\
&=&k+{k\over n}-1\\
&=&\Theta(\log n)
\end{eqnarray*}


Binary search trees (BST)

What is the average number of comparisons needed to insert $n$
distinct random elements into an initially empty BST?

$T(0)=T(1)=0$.

Assume that $A=(a_1,\ldots,a_n)$ is the list given and that 
$B=(b_1,\ldots,b_n)$ is $A$ sorted increasingly.
That $A$ is a random sequence implies that $a_1$ is equally likely
to be $b_j$ for any $1\le j\le n$. Consider the tree obtained after
the insertion of all $n$ numbers.

So $T(n)={1\over n}\sum_{j=1}^n(n-1+T(j-1)+T(n-j))=n-1+{2\over n}
\sum_{j=0}^{n-1}T(j)$ for $n\ge2$.

To solve this recurrence, we guess $T(n)=O(n\log n)$, i.e., 
$T(n)\le cn\ln n$ for some $c$. We prove by induction that
this is indeed the case.

When $n=1$, $T(n)=0\le c\cdot 1\cdot \ln 1$ for any $c$. So the
basis holds. Assume that when $j\le n-1$, $T(j)\le cj\ln j$.
Now consider $n$.
\begin{eqnarray*}
T(n)&=&n-1+{2\over n}\sum_{j=0}^{n-1}T(j)\\
&\le&n-1+{2\over n}\sum_{j=1}^{n-1}cj\ln j\\
&\le&n-1+{2\over n}c\int_2^n x\ln xdx\\
&\le&n-1+{2\over n}c({1\over2}n^2\ln n-{1\over4}n^2)\\
&=&n-1+cn\ln n-{c\over2}n\\
&\le&cn\ln n { \quad for\ \ } c\ge  2
\end{eqnarray*}


    \textbf{2.4 Amortized Analysis}

average over successive operations,
average the time per operation over a worst-case sequence.
It guarantees the average performance of each operation in the worst case.

Techniques:
(1) Aggregate analysis: get overall worst case $T(n)$, amortized cost per operation is $T(n) / n$.
(2) Accounting method: Represent prepaid work as credit stored with speciﬁc objects within the data
structure.
(3) Potential method (physics): Represent prepaid work as potential energy that can be released for future operations..
The potential is associated with the data structure as a whole rather than with speciﬁc objects within the data
structure

    \textbf{2.5 Probabilistic Analysis}

Similar to average case, but using randomly generated inputs instead of probabilistic distribution.


    \textbf{3. Data Structures}

    \input{ln3}


    \textbf{4. Greedy Method}

    ln4


    \textbf{5. Divide and Conquer}

    ln5


    \textbf{6. Dynamic Programming}

    \textbf{6.1 Introduction}

Divide and Conquer = top-down, recursive

Dynamic Programming = bottom-up, tabular,
store results of sub-problems to avoid re-computing

Eg. Fibonacci is $O(n)$ with DP, instead of $\Theta(1.618^n)$ with D\&C, since it stores previous answers

    \textbf{6.2 Making change} (example)

Let $n$, a positive integer, be the number of different
types of coin in a country. Let $coin[1..n]$, an array of
positive integers, be the values of these $n$ types of coin. Let
$m$, a positive integer, be the amount of change that one wishes to make.
Design a dynamic programming algorithm that determines whether $m$
can be made with the coins, and if so, computes the minimum number of coins
needed.

Define $count(i)$ to be the minimum number of coins to
make $i$ ($>0$). That $count(i)=\infty$ implies that no solution
exists. The recursive definition of $count(i)$ is as follows.

$count(1)=\infty$ if $1\not\in coin[\ ]$.

$count(coin[j])=1$ for $j=1,\ldots,n$.

$count(i)=1+\min_{1\le j\le n,coin[j]<i}\{count(i-coin[j])\}$

\item The table is a 1-D table and its entries are filled from left
to right until $count[m]$ is reached.

\begin{center}
\begin{tabular}{c|c|c|c|c}
$i$ & $1$ & $2$ & $\cdots$ & $m$\\\hline
$count[i]$ & $\rightarrow$ & $\rightarrow$ & $\cdots$ & $*$\\
\end{tabular}
\end{center}

Algorithm:

\qquad for $i\leftarrow 1$ to $m$ $count[i]\leftarrow -1$

\qquad $count[1]\leftarrow\infty$

\qquad for $j\leftarrow 1$ to $n$

\qquad\qquad $count[coin[j]]\leftarrow 1$

\qquad for $i\leftarrow 1$ to $m$

\qquad\qquad if $count[i]=-1$

\qquad\qquad\qquad $min\leftarrow\infty$

\qquad\qquad\qquad for $j\leftarrow 1$ to $n$

\qquad\qquad\qquad\qquad if $coin[j]<i$ 
              
\qquad\qquad\qquad\qquad\qquad if $min>count[i-coin[j]]$

\qquad\qquad\qquad\qquad\qquad\qquad $min\leftarrow count[i-coin[j]]$

\qquad\qquad\qquad $count[i]\leftarrow 1+min$
 
Time complexity: $\Theta(mn)$. (pseudo-polynomial)



    \textbf{7. The Lower Bound Theory}

    ln7


    \textbf{8. Online Algorithms}

    ln8
    

    % The content will fill out the three columns without the lipsum
    % Get rid of them when done
    \lipsum
    \lipsum

\end{multicols}
\end{flushleft}
\end{document}